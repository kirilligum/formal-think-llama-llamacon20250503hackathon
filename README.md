- # Formal Think Llama
    - https://github.com/kirilligum/formal-think-llama-llamacon20250503hackathon
      https://youtu.be/bBshVQEiQHU
      By Kirill Igumenshchev https://x.com/kirill_igum
    - the latest improvements towards AGI were due to synthetic data and ttc. GPQA: 
        - gpt4 50% came out 2 years ago. gpt4o is similar performance but cheaper**n**
        - gpt4.1 65% isn't much better than gpt4 that came out two years ago
        - o3 84% much better than current 4.1 but based on gpt4(o) that came out two years ago
        - llm hallucinate. if you ask them to think for 1000 tokens, they don't. if you ask them for a specific way of thinking (graph of thought) they often don't
    - we use formal verification technique -- automata to adjust probability and obey our constraints
        - [Adaptable Logical Control for Large Language Models](https://arxiv.org/abs/2406.13892) 
          Honghua Zhang &Po-Nien Kungâˆ— &Masahiro Yoshida Guy Van den Broeck &Nanyun Peng
          https://github.com/joshuacnf/Ctrl-G 
        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2FFTLJxxBadX.png?alt=media&token=c6f75151-7a8b-4a61-a470-7d17637ec289)
            - we multiply logpobs of llm on to a probability adjustment from hidden markov model that looks ahead and verifies the stat
        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2FNsZ0RAQfpX.png?alt=media&token=bea3cdd5-ddc7-4cb3-85fa-83db9af8a6d7)
            - 
    - it is not simply checking the token but running a hidden markov model that predicts a few tokens ahead and adjusts the probability of the next token generated by LLM
    - demo
        - without formal "step" state
            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2F_MFnyJ-TU2.png?alt=media&token=0bf218f5-7cdc-44ba-9701-d454d0741787)
            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2FoJj35TVXHL.png?alt=media&token=78a90742-c0a1-479e-969b-14561ae78525)
        - with formal "step" state
            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2FWHZ182ST-B.png?alt=media&token=56fe573f-08b8-4507-9007-259165829cd1)
            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fkirilligum-personal%2F8O1B1yb_t5.png?alt=media&token=5d835583-0733-47de-9f8b-5d4232433cad)
    - how we built it
        - followed "Adaptable Logical Control for Large Language Models" paper 
        - got English prompts out of https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset dataset using lingua-py
        - distilled llama 3.2 1b into a hmm take h100 80 GiB. went through a lot of failures adjusting parameters on h100 provided by nebius and lambda
        - constructed DFA for reasoning https://gemini.google.com/app/c486a0081ca575d3
        - ran the modified model that picks next token in LLM generation based on HMM lookahead
        - tried a few models, parameters, datasets
    - conclusion
        - lambda, llama, nebius are amazing
        - Llama 7b (Tulu2 7b) gets reasoning !!! and thinking budget!!!
        - Llama4 scout can distill to HMM but takes a lot of time on 8xh100. even llama3.2-1b is difficult on 1xh100
        - nemotron-cc is a good data source
        - giving a paper and its code to an LLM doesn't magically get you a practical application but helps to find dead-ends :-)
