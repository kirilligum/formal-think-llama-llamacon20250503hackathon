{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Distillation Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. sampling data from the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = 'gpt2-large'\n",
    "\n",
    "# a list of prompts used to sample data from the given LLM, \n",
    "# please change according to the LLM & your use cases.\n",
    "INPUT_FILE = './workspace/inference_data/distillation_prompts.json' \n",
    "\n",
    "DATASET = 'gpt2-large'\n",
    "DATA_PATH = f'./workspace/hmm_data/{DATASET}'\n",
    "LVD_SIZE = 100000\n",
    "CHUNK_SIZE = 100000\n",
    "DEV_SIZE = 20000\n",
    "TOTAL_CHUNKS = 100\n",
    "SEQUENCE_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --standalone --nproc_per_node=gpu     sample_data.py     --model_name_or_path gpt2-large     --tokenizer_name_or_path gpt2-large     --input_file ./workspace/inference_data/distillation_prompts.json --chunk_size 100000     --batch_size 512 --max_new_tokens 32     --save_embeddings --output_file ./workspace/hmm_data/gpt2-large/gpt2-large.lvd\n",
      "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --standalone --nproc_per_node=gpu     sample_data.py     --model_name_or_path gpt2-large     --tokenizer_name_or_path gpt2-large     --input_file ./workspace/inference_data/distillation_prompts.json --chunk_size 100000 --total_chunks 100     --batch_size 512 --max_new_tokens 32     --output_file ./workspace/hmm_data/gpt2-large/gpt2-large.train\n",
      "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --standalone --nproc_per_node=gpu     sample_data.py     --model_name_or_path gpt2-large     --tokenizer_name_or_path gpt2-large     --input_file ./workspace/inference_data/distillation_prompts.json --chunk_size 20000     --batch_size 512 --max_new_tokens 32     --output_file ./workspace/hmm_data/gpt2-large/gpt2-large.dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "CUDA_CORES = '0,1,2,3,4,5'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "# create data path\n",
    "os.system(f'mkdir -p {DATA_PATH}')\n",
    "\n",
    "\n",
    "# sample LVD_SIZE examples for initializing hmm parameters via latent variable distillation (LVD)\n",
    "cmd = f'CUDA_VISIBLE_DEVICES={CUDA_CORES} torchrun --standalone --nproc_per_node=gpu \\\n",
    "    sample_data.py \\\n",
    "    --model_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --tokenizer_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --input_file {INPUT_FILE} --chunk_size {LVD_SIZE} \\\n",
    "    --batch_size {BATCH_SIZE} --max_new_tokens {SEQUENCE_LEN} \\\n",
    "    --save_embeddings --output_file {DATA_PATH}/{DATASET}.lvd'.strip()\n",
    "print(cmd)\n",
    "\n",
    "\n",
    "# sample TOTAL_CHUNKS chunks of training examples as the training set\n",
    "cmd = f'CUDA_VISIBLE_DEVICES={CUDA_CORES} torchrun --standalone --nproc_per_node=gpu \\\n",
    "    sample_data.py \\\n",
    "    --model_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --tokenizer_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --input_file {INPUT_FILE} --chunk_size {CHUNK_SIZE} --total_chunks {TOTAL_CHUNKS} \\\n",
    "    --batch_size {BATCH_SIZE} --max_new_tokens {SEQUENCE_LEN} \\\n",
    "    --output_file {DATA_PATH}/{DATASET}.train'.strip()\n",
    "print(cmd)\n",
    "\n",
    "\n",
    "# sample DEV_SIZE examples as the dev set\n",
    "cmd = f'CUDA_VISIBLE_DEVICES={CUDA_CORES} torchrun --standalone --nproc_per_node=gpu \\\n",
    "    sample_data.py \\\n",
    "    --model_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --tokenizer_name_or_path {BASE_MODEL_PATH} \\\n",
    "    --input_file {INPUT_FILE} --chunk_size {DEV_SIZE} \\\n",
    "    --batch_size {BATCH_SIZE} --max_new_tokens {SEQUENCE_LEN} \\\n",
    "    --output_file {DATA_PATH}/{DATASET}.dev'.strip()\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. initialize checkpoint-0 for training HMM via latent variable distillation (LVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# specify the HMM size\u001b[39;00m\n\u001b[32m      5\u001b[39m HIDDEN_STATES = \u001b[32m4096\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# specify the HMM size\n",
    "HIDDEN_STATES = 4096\n",
    "\n",
    "# get vocab_size and eos_token_id; might vary for different models #\n",
    "__tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "VOCAB_SIZE = __tokenizer.vocab_size\n",
    "EOS_TOKEN_ID = __tokenizer.eos_token_id\n",
    "####################################################################\n",
    "\n",
    "HMM_MODEL_ID = f'hmm_{DATASET}_{HIDDEN_STATES}'\n",
    "HMM_MODEL_PATH = f'./workspace/models/{HMM_MODEL_ID}/'\n",
    "\n",
    "_ = os.system(f'mkdir -p {HMM_MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CUDA_CORES = '0,1,2,3,4,5'\n",
    "SEQUENCES_FILE = f'{DATA_PATH}/{DATASET}.lvd'\n",
    "EMEBEDDINGS_FILE = f'{DATA_PATH}/{DATASET}.lvd.embeddings'\n",
    "\n",
    "# latent variable distillation\n",
    "cmd = f'CUDA_VISIBLE_DEVICES={CUDA_CORES} python lvd_hmm.py \\\n",
    "    --sequences_file {SEQUENCES_FILE} --embeddings_file {EMEBEDDINGS_FILE} \\\n",
    "    --hidden_states {HIDDEN_STATES} --vocab_size {VOCAB_SIZE} --eos_token_id {EOS_TOKEN_ID} \\\n",
    "    --kmeans_iterations 100 --pseudocount 0.001 \\\n",
    "    --output_file {HMM_MODEL_PATH}/checkpoint-0'\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. train HMM via Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('mkdir -p ./workspace/logs')\n",
    "LOG_FILE=f'./workspace/logs/{HMM_MODEL_ID}_log.txt'\n",
    "\n",
    "CUDA_CORES = '0,1,2,3,4,5'\n",
    "BATCH_SIZE = 256\n",
    "SAVE_PER_STEP = 10\n",
    "DROPOUT = 0.01\n",
    "\n",
    "# EM training schedule:\n",
    "# 1. train for 10 EM steps, each step using 1 chunk of data\n",
    "# 2. train for 5 EM steps, each step using 2 chunks of data\n",
    "# 3. train for 4 EM steps, each step using 5 chunks of data\n",
    "# 4. train for 4 EM steps, each step using 10 chunks of data\n",
    "# 5. train for 4 EM steps, each step using 20 chunks of data\n",
    "# 6. train for 1 EM steps, each step using 40 chunks of data\n",
    "EM_SCHEDULE = \"\\\"10,1;5,2;4,5;4,10;4,20;1,40\\\"\"\n",
    "\n",
    "cmd = f'CUDA_VISIBLE_DEVICES={CUDA_CORES} torchrun --standalone --nproc_per_node=gpu train_hmm.py \\\n",
    "    --model_path {HMM_MODEL_PATH} --checkpoint 0 --save_per_step {SAVE_PER_STEP} \\\n",
    "    --data_path {DATA_PATH} --dataset {DATASET} --total_chunks {TOTAL_CHUNKS} --batch_size {BATCH_SIZE} \\\n",
    "    --em_schedule {EM_SCHEDULE} --dropout {DROPOUT} --log_file {LOG_FILE}'.strip()\n",
    "print(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
